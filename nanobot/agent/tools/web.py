"""Web tools: web_search and web_fetch."""

import html
import json
import os
import re
from typing import Any
from urllib.parse import urlparse

import httpx

from nanobot.agent.tools.base import Tool

# Shared constants
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_7_2) AppleWebKit/537.36"
MAX_REDIRECTS = 5  # Limit redirects to prevent DoS attacks


def _strip_tags(text: str) -> str:
    """Remove HTML tags and decode entities."""
    text = re.sub(r'<script[\s\S]*?</script>', '', text, flags=re.I)
    text = re.sub(r'<style[\s\S]*?</style>', '', text, flags=re.I)
    text = re.sub(r'<[^>]+>', '', text)
    return html.unescape(text).strip()


def _normalize(text: str) -> str:
    """Normalize whitespace."""
    text = re.sub(r'[ \t]+', ' ', text)
    return re.sub(r'\n{3,}', '\n\n', text).strip()


def _validate_url(url: str) -> tuple[bool, str]:
    """Validate URL: must be http(s) with valid domain."""
    try:
        p = urlparse(url)
        if p.scheme not in ('http', 'https'):
            return False, f"Only http/https allowed, got '{p.scheme or 'none'}'"
        if not p.netloc:
            return False, "Missing domain"
        return True, ""
    except Exception as e:
        return False, str(e)


_SEARCH_ENV_VARS = {
    "brave": "BRAVE_API_KEY",
    "tavily": "TAVILY_API_KEY",
    "gemini": "GEMINI_API_KEY",
    "grok": "XAI_API_KEY",
}


class WebSearchTool(Tool):
    """Search the web. Supports Brave, Tavily, and Gemini grounding."""

    name = "web_search"
    description = "Search the web. Returns titles, URLs, and snippets."
    parameters = {
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "Search query"},
            "count": {"type": "integer", "description": "Results (1-10)", "minimum": 1, "maximum": 10}
        },
        "required": ["query"]
    }

    _DEFAULT_MODELS = {
        "gemini": "gemini-2.5-flash",
        "grok": "grok-4-1-fast-reasoning",
    }

    def __init__(self, api_key: str | None = None, max_results: int = 5, provider: str = "brave", model: str = ""):
        self._init_api_key = api_key
        self.max_results = max_results
        self.provider = provider
        self.model = model

    @property
    def api_key(self) -> str:
        """Resolve API key at call time so env/config changes are picked up."""
        env_var = _SEARCH_ENV_VARS.get(self.provider, "BRAVE_API_KEY")
        return self._init_api_key or os.environ.get(env_var, "")

    async def execute(self, query: str, count: int | None = None, **kwargs: Any) -> str:
        if not self.api_key:
            env_var = _SEARCH_ENV_VARS.get(self.provider, "BRAVE_API_KEY")
            return (
                f"Error: {self.provider.title()} Search API key not configured. "
                f"Set it in ~/.nanobot/config.json under tools.web.search.apiKey "
                f"(or export {env_var}), then restart the gateway."
            )
        try:
            n = min(max(count or self.max_results, 1), 10)
            if self.provider == "tavily":
                return await self._search_tavily(query, n)
            elif self.provider == "gemini":
                return await self._search_gemini(query, n)
            elif self.provider == "grok":
                return await self._search_grok(query, n)
            else:
                return await self._search_brave(query, n)
        except Exception as e:
            return f"Error: {e}"

    async def _search_brave(self, query: str, n: int) -> str:
        async with httpx.AsyncClient() as client:
            r = await client.get(
                "https://api.search.brave.com/res/v1/web/search",
                params={"q": query, "count": n},
                headers={"Accept": "application/json", "X-Subscription-Token": self.api_key},
                timeout=10.0
            )
            r.raise_for_status()
        results = r.json().get("web", {}).get("results", [])
        if not results:
            return f"No results for: {query}"
        lines = [f"Results for: {query}\n"]
        for i, item in enumerate(results[:n], 1):
            lines.append(f"{i}. {item.get('title', '')}\n   {item.get('url', '')}")
            if desc := item.get("description"):
                lines.append(f"   {desc}")
        return "\n".join(lines)

    async def _search_tavily(self, query: str, n: int) -> str:
        async with httpx.AsyncClient() as client:
            r = await client.post(
                "https://api.tavily.com/search",
                json={"query": query, "max_results": n, "include_answer": True},
                headers={"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"},
                timeout=15.0
            )
            r.raise_for_status()
        data = r.json()
        results = data.get("results", [])
        if not results:
            return f"No results for: {query}"
        lines = [f"Results for: {query}\n"]
        if answer := data.get("answer"):
            lines.append(answer)
            lines.append("")
        for i, item in enumerate(results[:n], 1):
            lines.append(f"{i}. {item.get('title', '')}\n   {item.get('url', '')}")
            if content := item.get("content"):
                lines.append(f"   {content}")
        return "\n".join(lines)

    async def _search_gemini(self, query: str, n: int) -> str:
        model = self.model or self._DEFAULT_MODELS["gemini"]
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
        payload = {
            "contents": [{"parts": [{"text": query}]}],
            "tools": [{"google_search": {}}],
        }
        async with httpx.AsyncClient() as client:
            r = await client.post(url, params={"key": self.api_key}, json=payload, timeout=20.0)
            r.raise_for_status()
        data = r.json()
        candidates = data.get("candidates", [])
        if not candidates:
            return f"No results for: {query}"
        candidate = candidates[0]
        parts = candidate.get("content", {}).get("parts", [])
        ai_text = " ".join(p.get("text", "") for p in parts).strip()
        chunks = candidate.get("groundingMetadata", {}).get("groundingChunks", [])
        lines = [f"Results for: {query}\n"]
        if ai_text:
            lines.append(ai_text)
            lines.append("")
        if chunks:
            lines.append("Sources:")
            for i, chunk in enumerate(chunks[:n], 1):
                web = chunk.get("web", {})
                lines.append(f"{i}. {web.get('title', '')}\n   {web.get('uri', '')}")
        return "\n".join(lines)

    async def _search_grok(self, query: str, n: int) -> str:
        import asyncio
        
        payload = {
            "model": self.model or self._DEFAULT_MODELS["grok"],
            "input": [{"role": "user", "content": query}],
            "tools": [{"type": "web_search"}],
        }
        
        # Retry on rate limit (429)
        for attempt in range(3):
            try:
                async with httpx.AsyncClient() as client:
                    r = await client.post(
                        "https://api.x.ai/v1/responses",
                        json=payload,
                        headers={"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"},
                        timeout=30.0,
                    )
                    if r.status_code == 429:
                        retry_after = 1.0
                        try:
                            data = r.json()
                            retry_after = float(data.get("retry_after", 1.0))
                        except Exception:
                            pass
                        if attempt < 2:
                            await asyncio.sleep(retry_after)
                            continue
                        return f"Error: Grok API rate limit exceeded. Please try again later."
                    r.raise_for_status()
                    data = r.json()
                    break
            except httpx.HTTPStatusError as e:
                if attempt == 2:
                    return f"Error: Grok API request failed: {e.response.status_code}"
                await asyncio.sleep(1)
            except Exception as e:
                if attempt == 2:
                    return f"Error: {e}"
                await asyncio.sleep(1)
        
        # Extract text from output array (Responses API format)
        ai_text = ""
        for item in data.get("output", []):
            if item.get("type") == "message":
                for part in item.get("content", []):
                    if part.get("type") == "output_text":
                        ai_text = part.get("text", "")
                        break
        citations = data.get("citations", [])
        lines = [f"Results for: {query}\n"]
        if ai_text:
            lines.append(ai_text)
            lines.append("")
        if citations:
            lines.append("Sources:")
            for i, c in enumerate(citations[:n], 1):
                if isinstance(c, dict):
                    lines.append(f"{i}. {c.get('title', '')}\n   {c.get('url', '')}")
                elif isinstance(c, str):
                    lines.append(f"{i}. {c}")
        return "\n".join(lines)


class WebFetchTool(Tool):
    """Fetch and extract content from a URL using Readability."""
    
    name = "web_fetch"
    description = "Fetch URL and extract readable content (HTML â†’ markdown/text)."
    parameters = {
        "type": "object",
        "properties": {
            "url": {"type": "string", "description": "URL to fetch"},
            "extractMode": {"type": "string", "enum": ["markdown", "text"], "default": "markdown"},
            "maxChars": {"type": "integer", "minimum": 100}
        },
        "required": ["url"]
    }
    
    def __init__(self, max_chars: int = 50000):
        self.max_chars = max_chars
    
    async def execute(self, url: str, extractMode: str = "markdown", maxChars: int | None = None, **kwargs: Any) -> str:
        from readability import Document

        max_chars = maxChars or self.max_chars

        # Validate URL before fetching
        is_valid, error_msg = _validate_url(url)
        if not is_valid:
            return json.dumps({"error": f"URL validation failed: {error_msg}", "url": url}, ensure_ascii=False)

        try:
            async with httpx.AsyncClient(
                follow_redirects=True,
                max_redirects=MAX_REDIRECTS,
                timeout=30.0
            ) as client:
                r = await client.get(url, headers={"User-Agent": USER_AGENT})
                r.raise_for_status()
            
            ctype = r.headers.get("content-type", "")
            
            # JSON
            if "application/json" in ctype:
                text, extractor = json.dumps(r.json(), indent=2, ensure_ascii=False), "json"
            # HTML
            elif "text/html" in ctype or r.text[:256].lower().startswith(("<!doctype", "<html")):
                doc = Document(r.text)
                content = self._to_markdown(doc.summary()) if extractMode == "markdown" else _strip_tags(doc.summary())
                text = f"# {doc.title()}\n\n{content}" if doc.title() else content
                extractor = "readability"
            else:
                text, extractor = r.text, "raw"
            
            truncated = len(text) > max_chars
            if truncated:
                text = text[:max_chars]
            
            return json.dumps({"url": url, "finalUrl": str(r.url), "status": r.status_code,
                              "extractor": extractor, "truncated": truncated, "length": len(text), "text": text}, ensure_ascii=False)
        except Exception as e:
            return json.dumps({"error": str(e), "url": url}, ensure_ascii=False)
    
    def _to_markdown(self, html: str) -> str:
        """Convert HTML to markdown."""
        # Convert links, headings, lists before stripping tags
        text = re.sub(r'<a\s+[^>]*href=["\']([^"\']+)["\'][^>]*>([\s\S]*?)</a>',
                      lambda m: f'[{_strip_tags(m[2])}]({m[1]})', html, flags=re.I)
        text = re.sub(r'<h([1-6])[^>]*>([\s\S]*?)</h\1>',
                      lambda m: f'\n{"#" * int(m[1])} {_strip_tags(m[2])}\n', text, flags=re.I)
        text = re.sub(r'<li[^>]*>([\s\S]*?)</li>', lambda m: f'\n- {_strip_tags(m[1])}', text, flags=re.I)
        text = re.sub(r'</(p|div|section|article)>', '\n\n', text, flags=re.I)
        text = re.sub(r'<(br|hr)\s*/?>', '\n', text, flags=re.I)
        return _normalize(_strip_tags(text))
